{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional exercise #1  (CSE 628) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import bigrams, trigrams\n",
    "import string\n",
    "from decimal import *\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Setting decimal point precision\n",
    "getcontext().prec = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 1000 sentences and 2882 unique words\n",
      "Size of test set: 1000 sentences and 2611 unqiue words\n"
     ]
    }
   ],
   "source": [
    "#Importing data\n",
    "new_data = []\n",
    "data = gutenberg.sents('austen-sense.txt')\n",
    "data = data[2:]\n",
    "\n",
    "#Add sentence de-limiters, remove punctuations and switch to lower-case\n",
    "for d in data:\n",
    "    d = [''.join(c for c in s if c not in string.punctuation) for s in d]\n",
    "    d = [s for s in d if s]\n",
    "    d.insert(0,'<s>')\n",
    "    d.append('</s>')\n",
    "    d = [str(w.lower()) for w in d]\n",
    "    new_data.append(d)\n",
    "    \n",
    "#The first 1000 sentences go into training set\n",
    "train_set = new_data[0:1000]\n",
    "#The second 1000 sentences go into test set\n",
    "test_set = new_data[1000:2000]\n",
    "\n",
    "train_words = [val for sublist in train_set for val in sublist]\n",
    "test_words = [val for sublist in test_set for val in sublist]\n",
    "num_words = len(train_words)\n",
    "\n",
    "print \"Size of training set: {} sentences and {} unique words\".format(len(train_set), len(set(train_words)))\n",
    "print \"Size of test set: {} sentences and {} unqiue words\".format(len(test_set), len(set([val for sublist in test_set for val in sublist])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Training and evaluating N-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams: 2882\n",
      "Number of bigrams: 14341\n",
      "Number of trigrams: 21306\n"
     ]
    }
   ],
   "source": [
    "#Calculating bigrams\n",
    "train_bigrams = list(bigrams(train_words))\n",
    "#Calculating trigrams\n",
    "train_trigrams = list(trigrams(train_words))\n",
    "condition_pairs = [((w0, w1), w2) for w0, w1, w2 in train_trigrams]\n",
    "\n",
    "#Calculating frequency/conditional frequency for N-grams\n",
    "uni_freq = nltk.FreqDist(train_words)\n",
    "bi_cfreq = nltk.ConditionalFreqDist(train_bigrams)\n",
    "tri_cfreq = nltk.ConditionalFreqDist(condition_pairs)\n",
    "\n",
    "#Calculating frequency/conditional probability for N-grams\n",
    "uni_prob = {}\n",
    "for k,v in uni_freq.iteritems():\n",
    "    uni_prob[k] = Decimal(v)/Decimal(num_words)\n",
    "    \n",
    "bi_prob = nltk.ConditionalProbDist(bi_cfreq, nltk.MLEProbDist)\n",
    "tri_prob = nltk.ConditionalProbDist(tri_cfreq, nltk.MLEProbDist)\n",
    "\n",
    "print \"Number of unigrams: {}\".format(len(set(train_words)))\n",
    "print \"Number of bigrams: {}\".format(len(set(train_bigrams)))\n",
    "print \"Number of trigrams: {}\".format(len(set(train_trigrams)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Observation\n",
    "We can observe that the number of unqiue N-grams increases on increasing the value of N. This is because, eg - repeating 3-word combinations (or trigrams) are rarer than repeating 2-word combinations (or bigrams), which in turn are rarer than repeating single words (or unigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to unigram model: 388/1000\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE on test sentences using unigram model\n",
    "test_uni_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    uni_mle = 1\n",
    "    for t in test_set[i][1:]:\n",
    "        if t in uni_prob.keys():\n",
    "            uni_mle *= uni_prob[t]\n",
    "        else:\n",
    "            uni_mle = 0.0\n",
    "            break\n",
    "    test_uni_mle.append(uni_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to unigram model: {}/{}\".format(\n",
    "    np.count_nonzero(test_uni_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to bigram model: 29/1000\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE on test sentences using bigram model\n",
    "test_bi_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    bi_mle = 1\n",
    "    test_bigrams = list(bigrams(test_set[i]))\n",
    "    for j in range(len(test_bigrams)):\n",
    "        if test_bigrams[j][0] in bi_prob.keys():\n",
    "            bi_mle *= Decimal(bi_prob[test_bigrams[j][0]].prob(test_bigrams[j][1]))\n",
    "        else:\n",
    "            bi_mle = 0.0\n",
    "            break\n",
    "    test_bi_mle.append(bi_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to bigram model: {}/{}\".format(\n",
    "    np.count_nonzero(test_bi_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to trigram model: 16/1000\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE on test sentences using trigram model\n",
    "test_tri_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    tri_mle = 1\n",
    "    test_trigrams = list(trigrams(test_set[i]))\n",
    "    #if sentence only has less than 3 words, meaning no trigrams\n",
    "    if len(test_trigrams) == 0:\n",
    "        tri_mle = 0.0\n",
    "        continue\n",
    "    for j in range(len(test_trigrams)):\n",
    "        if (test_trigrams[j][0],test_trigrams[j][1]) in tri_prob.keys():\n",
    "            tri_mle *= Decimal(tri_prob[test_trigrams[j][0],test_trigrams[j][1]].prob(test_trigrams[j][2]))\n",
    "        else:\n",
    "            tri_mle = 0.0\n",
    "            break\n",
    "    test_tri_mle.append(tri_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to trigram model: {}/{}\".format(\n",
    "    np.count_nonzero(test_tri_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Observation **\n",
    "\n",
    "As expected, the number of probable N-grams in the test set decrease with increasing value of N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculating MLEs for randomly sampled sentences using N-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated random sentences:\n",
      "<s> end from a said mrs and a </s>\n",
      "<s> whose the not in </s>\n",
      "<s> delay applauded step that alone they out than cloud </s>\n",
      "<s> and without she a the far </s>\n",
      "<s> being conjectures cowper but of </s>\n"
     ]
    }
   ],
   "source": [
    "#Making up 5 non-sensical sentences from training set and calculating their MLE estimates\n",
    "all_rands = []\n",
    "sampling_uni_list = list(uni_prob.keys())\n",
    "\n",
    "print \"Generated random sentences:\"\n",
    "for i in range(5):\n",
    "    #Select sentence length at random\n",
    "    sent_len = random.sample(range(5,10),1)\n",
    "    rand_sent = []\n",
    "    for j in range(sent_len[0]):\n",
    "        sample = np.random.multinomial(1,uni_prob.values(),size=1)\n",
    "        if sampling_uni_list[np.nonzero(sample[0])[0][0]] != '<s>' and sampling_uni_list[np.nonzero(sample[0])[0][0]] != '</s>':\n",
    "            rand_sent.append(sampling_uni_list[np.nonzero(sample[0])[0][0]])\n",
    "    rand_sent.insert(0,'<s>')\n",
    "    rand_sent.append('</s>')\n",
    "    all_rands.append(rand_sent)\n",
    "    print ' '.join(rand_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram MLEs for the random sentences: \n",
      "1. 4.06763E-18\n",
      "2. 7.35321E-11\n",
      "3. 1.65396E-32\n",
      "4. 4.68780E-15\n",
      "5. 1.61975E-16\n",
      "\n",
      "Bigram MLEs for the random sentences: \n",
      "1. 0.0\n",
      "2. 0.0\n",
      "3. 0.0\n",
      "4. 0.0\n",
      "5. 0.0\n",
      "\n",
      "Trigram MLEs for the random sentences: \n",
      "1. 0.0\n",
      "2. 0.0\n",
      "3. 0.0\n",
      "4. 0.0\n",
      "5. 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_uni_mle = []\n",
    "for i in range(len(all_rands)):\n",
    "    uni_mle = 1\n",
    "    for t in all_rands[i][1:]:\n",
    "        if t in uni_prob.keys():\n",
    "            uni_mle *= uni_prob[t]\n",
    "        else:\n",
    "            uni_mle = 0.0\n",
    "            break\n",
    "    rand_uni_mle.append(uni_mle)\n",
    "\n",
    "print \"Unigram MLEs for the random sentences: \\n1. {}\\n2. {}\\n3. {}\\n4. {}\\n5. {}\\n\".format(rand_uni_mle[0], \n",
    "                                                rand_uni_mle[1], rand_uni_mle[2], rand_uni_mle[3], rand_uni_mle[4])\n",
    "\n",
    "rand_bi_mle = []\n",
    "for i in range(len(all_rands)):\n",
    "    bi_mle = 1\n",
    "    rand_bigrams = list(bigrams(all_rands[i]))\n",
    "    for j in range(len(rand_bigrams)):\n",
    "        if rand_bigrams[j][0] in bi_prob.keys():\n",
    "            bi_mle *= bi_prob[rand_bigrams[j][0]].prob(rand_bigrams[j][1])\n",
    "        else:\n",
    "            bi_mle = 0.0\n",
    "            break\n",
    "    rand_bi_mle.append(bi_mle)\n",
    "\n",
    "print \"Bigram MLEs for the random sentences: \\n1. {}\\n2. {}\\n3. {}\\n4. {}\\n5. {}\\n\".format(rand_bi_mle[0], \n",
    "                                                rand_bi_mle[1], rand_bi_mle[2], rand_bi_mle[3], rand_bi_mle[4])\n",
    "\n",
    "rand_tri_mle = []\n",
    "for i in range(len(all_rands)):\n",
    "    tri_mle = 1\n",
    "    rand_trigrams = list(trigrams(all_rands[i]))\n",
    "    #if sentence only has less than 3 words, meaning no trigrams\n",
    "    if len(rand_trigrams) == 0:\n",
    "        tri_mle = 0.0\n",
    "        continue\n",
    "    for j in range(len(rand_trigrams)):\n",
    "        if (rand_trigrams[j][0],rand_trigrams[j][1]) in tri_prob.keys():\n",
    "            tri_mle *= tri_prob[rand_trigrams[j][0],rand_trigrams[j][1]].prob(rand_trigrams[j][2])\n",
    "        else:\n",
    "            tri_mle = 0.0\n",
    "            break\n",
    "    rand_tri_mle.append(tri_mle)\n",
    "\n",
    "print \"Trigram MLEs for the random sentences: \\n1. {}\\n2. {}\\n3. {}\\n4. {}\\n5. {}\\n\".format(rand_tri_mle[0], \n",
    "                                            rand_tri_mle[1], rand_tri_mle[2], rand_tri_mle[3], rand_tri_mle[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ADD - 1 SMOOTHING\n",
    "### 3.1 Unigram Model\n",
    "As the words unseen in training set and occuring in the test set will result in 0 MLE, we take the following steps to prevent that - \n",
    "* Add a new token in the dictionary (formed from training set) called 'UNK', which will represent all the never seen before words in encountered in the test set.\n",
    "* Add 1 to the counts of all the words in this modified dictionary.\n",
    "* The new formula of MLE will now be: \n",
    "$$Pr_{MLE}(w_i) = \\dfrac{count(w_i) + 1}{\\Sigma_{w_j \\in V}count{(w_j)} + V}$$\n",
    "where V is the total number of unique unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to unigram model (add-1 smoothing): 1000/1000\n"
     ]
    }
   ],
   "source": [
    "#Implementing add-1 smoothing with unigram model. V = #unigram words\n",
    "\n",
    "#Adding 1 to the counts of existing entries\n",
    "add1_unifreq = uni_freq.copy()\n",
    "\n",
    "for k,v in add1_unifreq.iteritems():\n",
    "    add1_unifreq[k] = v + 1\n",
    "\n",
    "#Adding UNK token\n",
    "add1_unifreq['UNK'] = 1\n",
    "\n",
    "add1_uniprob = {}\n",
    "for k,v in add1_unifreq.iteritems():\n",
    "    add1_uniprob[k] = Decimal(v)/Decimal(sum(add1_unifreq.values()))\n",
    "\n",
    "add1_uni_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    uni_mle = 1\n",
    "    for t in test_set[i][1:]:\n",
    "        if t in add1_uniprob.keys():\n",
    "            uni_mle *= add1_uniprob[t]\n",
    "        else:\n",
    "            uni_mle *= add1_uniprob['UNK']\n",
    "            break\n",
    "    add1_uni_mle.append(uni_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to unigram model (add-1 smoothing): {}/{}\".format(\n",
    "    np.count_nonzero(add1_uni_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bigram Model\n",
    "* Similarly all the bigrams appearing in test set which didn't appear in the training set are represented by the 'UNK' token. Add 1 to the counts of all the tokens in our new vocabulary.\n",
    "* The new formula for MLE will now be:\n",
    "$$ Pr(w_j|w_i) = \\dfrac{count(w_i,w_j) + 1}{count(w_i) + V}$$\n",
    "where V is the total number of unique unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to bigram model (add-1 smoothing): 1000/1000\n"
     ]
    }
   ],
   "source": [
    "#Implementing add-1 smoothing with bigram model.\n",
    "\n",
    "all_test_bigrams = list(bigrams(test_words))\n",
    "#Adding 1 to the counts of existing entries\n",
    "add1_bifreq = nltk.ConditionalFreqDist(train_bigrams)\n",
    "voc = len(add1_unifreq)\n",
    "\n",
    "for b in all_test_bigrams:\n",
    "    if add1_bifreq[b[0]][b[1]] != 0:\n",
    "        add1_bifreq[b[0]][b[1]] += 1\n",
    "\n",
    "\n",
    "#Calculating new probability distribution\n",
    "add1_biprob = nltk.ConditionalProbDist(add1_bifreq, nltk.MLEProbDist)\n",
    "\n",
    "add1_bi_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    bi_mle = 1\n",
    "    test_bigrams = list(bigrams(test_set[i]))\n",
    "    for j in range(len(test_bigrams)):\n",
    "        if test_bigrams[j][0] in add1_biprob.keys() and  add1_biprob[test_bigrams[j][0]].prob(test_bigrams[j][1]) != 0:\n",
    "            bi_mle *= Decimal(add1_biprob[test_bigrams[j][0]].prob(test_bigrams[j][1]))\n",
    "        elif test_bigrams[j][0] in add1_unifreq.keys():\n",
    "            bi_mle *= Decimal(1)/Decimal(add1_unifreq[test_bigrams[j][0]]+voc)\n",
    "        else:\n",
    "            bi_mle *= Decimal(1)/Decimal(add1_unifreq['UNK']+voc)\n",
    "\n",
    "    add1_bi_mle.append(bi_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to bigram model (add-1 smoothing): {}/{}\".format(\n",
    "    np.count_nonzero(add1_bi_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Trigram Model\n",
    "* Again, trigrams appearing in test set which didn't appear in the training set are represented by the 'UNK' token. Add 1 to the counts of all the tokens in our new vocabulary.\n",
    "* The new formula for MLE will now be:\n",
    "$$ Pr(w_i|w_{i-2},w_{i-1}) = \\dfrac{count(w_{i-2},w_{i-1},w_i) + 1}{count(w_{i-2},w_{i-1}) + V}$$\n",
    "where V is the total number of unique bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to bigram model (add-1 smoothing): 1000/1000\n"
     ]
    }
   ],
   "source": [
    "#Implementing add-1 smoothing with trigram model.\n",
    "\n",
    "all_test_trigrams = list(trigrams(test_words))\n",
    "#Adding 1 to the counts of existing entries\n",
    "add1_trifreq = nltk.ConditionalFreqDist(condition_pairs)\n",
    "voc = len(set(train_bigrams))\n",
    "\n",
    "for b in all_test_trigrams:\n",
    "    if add1_trifreq[b[0],b[1]][b[2]] != 0:\n",
    "        add1_trifreq[b[0],b[1]][b[2]] += 1\n",
    "\n",
    "\n",
    "#Calculating new probability distribution\n",
    "add1_triprob = nltk.ConditionalProbDist(add1_trifreq, nltk.MLEProbDist)\n",
    "\n",
    "add1_tri_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    tri_mle = 1\n",
    "    test_trigrams = list(trigrams(test_set[i]))\n",
    "    for j in range(len(test_trigrams)):\n",
    "        if (test_trigrams[j][0],test_trigrams[j][1]) in add1_triprob.keys() and  add1_triprob[test_trigrams[j][0],test_trigrams[j][1]].prob(test_trigrams[j][2]) != 0:\n",
    "            tri_mle *= Decimal(add1_triprob[test_trigrams[j][0],test_trigrams[j][1]].prob(test_trigrams[j][2]))\n",
    "        elif test_trigrams[j][0] in add1_bifreq.keys():\n",
    "            tri_mle *= Decimal(1)/Decimal(add1_bifreq[test_trigrams[j][0]][test_trigrams[j][1]]+voc)\n",
    "        else:\n",
    "            tri_mle *= Decimal(1)/Decimal(add1_unifreq['UNK']+voc)\n",
    "\n",
    "    add1_tri_mle.append(tri_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to bigram model (add-1 smoothing): {}/{}\".format(\n",
    "    np.count_nonzero(add1_bi_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "The discount value increase with increasing value of N (of N-gram). This can be because the vocabulary on N-gram follows the order $\\#trigrams > \\#bigrams >> \\#unigrams$.\n",
    "This implies that the possibility of encountering an unseen N-gram is higher for higher value of N, and so more discounting has to be done. Therefore, $ discount_{trigram} > discount_{bigram} >> discount_{unigram} $.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Language Generation using N-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> must replied my him note but and habit future calmness chance innate very might soon only some called warmly </s>\n"
     ]
    }
   ],
   "source": [
    "#Generating sentencces using unigram model\n",
    "sampling_uni_list = list(uni_prob.keys())\n",
    "\n",
    "sent = ['<s>']\n",
    "sample = np.random.multinomial(1,uni_prob.values(),size=1)\n",
    "while sampling_uni_list[np.nonzero(sample[0])[0][0]] != '</s>':\n",
    "    if sampling_uni_list[np.nonzero(sample[0])[0][0]] != '<s>' and sampling_uni_list[np.nonzero(sample[0])[0][0]] !='</s>':\n",
    "        sent.append(sampling_uni_list[np.nonzero(sample[0])[0][0]])\n",
    "    sample = np.random.multinomial(1,uni_prob.values(),size=1)\n",
    "\n",
    "sent.append('</s>')\n",
    "print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> this circumstance related to hear the violence of norland had opportunity till she saw the weather they might perhaps it as perfect with the impertinent remarks do for them a piece and there must buy another name was not draw him to move when is your mother were now viewing you in law for them with respect improve this time for in the indifference of an alarm on each of every year amongst those who were then really love </s>\n"
     ]
    }
   ],
   "source": [
    "#Generating sentencces using bigram model\n",
    "sent = ['<s>']\n",
    "while sent[-1] != '</s>':\n",
    "    sent.append(bi_prob[sent[-1]].generate())\n",
    "\n",
    "print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> the situation of barton in a manner as to a stable to receive more enjoyment from them than she might think necessary if the sum were diminished one half five hundred pounds willoughby says would make it one of her eldest sister when they first came into devonshire to prepare the house with so little </s>\n"
     ]
    }
   ],
   "source": [
    "#Generating sentencces using trigram model\n",
    "sent = ['<s>']\n",
    "sent.append(bi_prob[sent[-1]].generate())\n",
    "while sent[-1] != '</s>':\n",
    "    sent.append(tri_prob[sent[-2],sent[-1]].generate())\n",
    "\n",
    "print ' '.join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "As can be seen, sentences generted using trigram model have more context than those generated using bigram model, which in turn have more context than sentences generated by the unigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
