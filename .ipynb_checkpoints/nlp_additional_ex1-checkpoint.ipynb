{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional exercise #1  (CSE 628) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import bigrams, trigrams\n",
    "import string\n",
    "from decimal import *\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#Setting decimal point precision\n",
    "getcontext().prec = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 1000 sentences and 2882 unique words\n",
      "Size of test set: 1000 sentences and 2611 unqiue words\n"
     ]
    }
   ],
   "source": [
    "#Importing data\n",
    "new_data = []\n",
    "data = gutenberg.sents('austen-sense.txt')\n",
    "data = data[2:]\n",
    "\n",
    "#Add sentence de-limiters, remove punctuations and switch to lower-case\n",
    "for d in data:\n",
    "    d = [''.join(c for c in s if c not in string.punctuation) for s in d]\n",
    "    d = [s for s in d if s]\n",
    "    d.insert(0,'<s>')\n",
    "    d.append('</s>')\n",
    "    d = [str(w.lower()) for w in d]\n",
    "    new_data.append(d)\n",
    "    \n",
    "#The first 1000 sentences go into training set\n",
    "train_set = new_data[0:1000]\n",
    "#The second 1000 sentences go into test set\n",
    "test_set = new_data[1000:2000]\n",
    "\n",
    "train_words = [val for sublist in train_set for val in sublist]\n",
    "num_words = len(train_words)\n",
    "\n",
    "print \"Size of training set: {} sentences and {} unique words\".format(len(train_set), len(set(train_words)))\n",
    "print \"Size of test set: {} sentences and {} unqiue words\".format(len(test_set), len(set([val for sublist in test_set for val in sublist])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unigrams: 2882\n",
      "Number of bigrams: 14341\n",
      "Number of trigrams: 21306\n"
     ]
    }
   ],
   "source": [
    "#Calculating bigrams\n",
    "train_bigrams = list(bigrams(train_words))\n",
    "#Calculating trigrams\n",
    "train_trigrams = list(trigrams(train_words))\n",
    "\n",
    "#Counting unigrams, bigrams and trigrams encountered in the training set\n",
    "uni_count = dict([(item, train_words.count(item)) for item in sorted(set(train_words))])\n",
    "bi_count = dict([(item, train_bigrams.count(item)) for item in sorted(set(train_bigrams))])\n",
    "tri_count = dict([(item, train_trigrams.count(item)) for item in sorted(set(train_trigrams))])\n",
    "\n",
    "print \"Number of unigrams: {}\".format(len(uni_count))\n",
    "print \"Number of bigrams: {}\".format(len(bi_count))\n",
    "print \"Number of trigrams: {}\".format(len(tri_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Observation\n",
    "We can observe that the number of unqiue N-grams increases on increasing the value of N. This is because, eg - repeating 3-word combinations (or trigrams) are rarer than repeating 2-word combinations (or bigrams), which in turn are rarer than repeating single words (or unigrams)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to unigram model: 388/1000\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE on test sentences using unigram model\n",
    "test_uni_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    uni_mle = 1\n",
    "    for t in test_set[i][1:]:\n",
    "        if t in uni_count.keys():\n",
    "            uni_mle *= Decimal(uni_count[t])/Decimal(num_words)\n",
    "        else:\n",
    "            uni_mle = 0\n",
    "            break\n",
    "    test_uni_mle.append(uni_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to unigram model: {}/{}\".format(\n",
    "    np.count_nonzero(test_uni_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to bigram model: 29/1000\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE on test sentences using bigram model\n",
    "test_bi_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    bi_mle = 1\n",
    "    test_bigrams = list(bigrams(test_set[i]))\n",
    "    for j in range(len(test_bigrams)):\n",
    "        if test_bigrams[j] in bi_count.keys():\n",
    "            bi_mle *= Decimal(bi_count[test_bigrams[j]])/Decimal(uni_count[test_bigrams[j][0]])\n",
    "        else:\n",
    "            bi_mle = 0\n",
    "            break\n",
    "    test_bi_mle.append(bi_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to bigram model: {}/{}\".format(\n",
    "    np.count_nonzero(test_bi_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to trigram model: 16/1000\n"
     ]
    }
   ],
   "source": [
    "#Calculating MLE on test sentences using trigram model\n",
    "test_tri_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    tri_mle = 1\n",
    "    test_trigrams = list(trigrams(test_set[i]))\n",
    "    #if sentence only has less than 3 words, meaning no trigrams\n",
    "    if len(test_trigrams) == 0:\n",
    "        tri_mle = 0\n",
    "        continue\n",
    "    for j in range(len(test_trigrams)):\n",
    "        if test_trigrams[j] in tri_count.keys():\n",
    "            tri_mle *= Decimal(tri_count[test_trigrams[j]])/Decimal(bi_count[(test_trigrams[j][0],\n",
    "                                                                              test_trigrams[j][1])])\n",
    "        else:\n",
    "            tri_mle = 0\n",
    "            break\n",
    "    test_tri_mle.append(tri_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to trigram model: {}/{}\".format(\n",
    "    np.count_nonzero(test_tri_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated random sentences:\n",
      "<s> embarrassment sail meet affections 8 laugh </s>\n",
      "<s> stone read rent eat aggrandizement enquiry preserver 8 deficiencies </s>\n",
      "<s> appearance eat reflecting compass argument </s>\n",
      "<s> stone power preserver aggrandizement gradually </s>\n",
      "<s> already rival insipidity gradually mistake staircase </s>\n"
     ]
    }
   ],
   "source": [
    "#Building multinomial probability distribution for uni, bi and trigrams\n",
    "sampling_uni_count = uni_count.copy()\n",
    "del sampling_uni_count['<s>']\n",
    "del sampling_uni_count['</s>']\n",
    "sampling_uni_list = list(sampling_uni_count.keys())\n",
    "prob_uni = {k: Decimal(v) / Decimal(sum(sampling_uni_count.values())) for k, v in sampling_uni_count.iteritems()}\n",
    "#prob_bi = {k: Decimal(v) / Decimal(uni_count[k[0]]) for k, v in bi_count.iteritems()}\n",
    "#prob_tri = {k: Decimal(v) / Decimal(bi_count[k[0],k[1]]) for k, v in tri_count.iteritems()}\n",
    "\n",
    "#Making up 5 non-sensical sentences from training set and calculating their MLE estimates\n",
    "all_rands = []\n",
    "print \"Generated random sentences:\"\n",
    "for i in range(5):\n",
    "    #Select sentence length at random\n",
    "    sent_len = random.sample(range(5,10),1)\n",
    "    rand_sent = []\n",
    "    for j in range(sent_len[0]):\n",
    "        sample = np.random.multinomial(1,prob_uni.values(),size=1)\n",
    "        rand_sent.append(sampling_uni_list[np.nonzero(sample[0])[0][0]])\n",
    "    rand_sent.insert(0,'<s>')\n",
    "    rand_sent.append('</s>')\n",
    "    all_rands.append(rand_sent)\n",
    "    print ' '.join(rand_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram MLEs for the random sentences: \n",
      "1. 1.51805E-27\n",
      "2. 1.55200E-40\n",
      "3. 1.85816E-23\n",
      "4. 5.10996E-23\n",
      "5. 3.79514E-27\n",
      "\n",
      "Bigram MLEs for the random sentences: \n",
      "1. 0\n",
      "2. 0\n",
      "3. 0\n",
      "4. 0\n",
      "5. 0\n",
      "\n",
      "Trigram MLEs for the random sentences: \n",
      "1. 0\n",
      "2. 0\n",
      "3. 0\n",
      "4. 0\n",
      "5. 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rand_uni_mle = []\n",
    "for i in range(len(all_rands)):\n",
    "    uni_mle = 1\n",
    "    for t in all_rands[i][1:]:\n",
    "        if t in uni_count.keys():\n",
    "            uni_mle *= Decimal(uni_count[t])/Decimal(num_words)\n",
    "        else:\n",
    "            uni_mle = 0\n",
    "            break\n",
    "    rand_uni_mle.append(uni_mle)\n",
    "\n",
    "print \"Unigram MLEs for the random sentences: \\n1. {}\\n2. {}\\n3. {}\\n4. {}\\n5. {}\\n\".format(rand_uni_mle[0], \n",
    "                                                rand_uni_mle[1], rand_uni_mle[2], rand_uni_mle[3], rand_uni_mle[4])\n",
    "\n",
    "rand_bi_mle = []\n",
    "for i in range(len(all_rands)):\n",
    "    bi_mle = 1\n",
    "    rand_bigrams = list(bigrams(all_rands[i]))\n",
    "    for j in range(len(rand_bigrams)):\n",
    "        if rand_bigrams[j] in bi_count.keys():\n",
    "            bi_mle *= Decimal(bi_count[rand_bigrams[j]])/Decimal(uni_count[rand_bigrams[j][0]])\n",
    "        else:\n",
    "            bi_mle = 0\n",
    "            break\n",
    "    rand_bi_mle.append(bi_mle)\n",
    "\n",
    "print \"Bigram MLEs for the random sentences: \\n1. {}\\n2. {}\\n3. {}\\n4. {}\\n5. {}\\n\".format(rand_bi_mle[0], \n",
    "                                                rand_bi_mle[1], rand_bi_mle[2], rand_bi_mle[3], rand_bi_mle[4])\n",
    "\n",
    "rand_tri_mle = []\n",
    "for i in range(len(all_rands)):\n",
    "    tri_mle = 1\n",
    "    rand_trigrams = list(trigrams(all_rands[i]))\n",
    "    #if sentence only has less than 3 words, meaning no trigrams\n",
    "    if len(rand_trigrams) == 0:\n",
    "        tri_mle = 0\n",
    "        continue\n",
    "    for j in range(len(rand_trigrams)):\n",
    "        if rand_trigrams[j] in tri_count.keys():\n",
    "            tri_mle *= Decimal(tri_count[rand_trigrams[j]])/Decimal(bi_count[(rand_trigrams[j][0],\n",
    "                                                                              rand_trigrams[j][1])])\n",
    "        else:\n",
    "            tri_mle = 0\n",
    "            break\n",
    "    rand_tri_mle.append(tri_mle)\n",
    "\n",
    "print \"Trigram MLEs for the random sentences: \\n1. {}\\n2. {}\\n3. {}\\n4. {}\\n5. {}\\n\".format(rand_tri_mle[0], \n",
    "                                            rand_tri_mle[1], rand_tri_mle[2], rand_tri_mle[3], rand_tri_mle[4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ADD - 1 SMOOTHING\n",
    "### Unigram Model\n",
    "As the words unseen in training set and occuring in the test set will result in 0 MLE, we take the following steps to prevent that - \n",
    "* Add a new token in the dictionary (formed from training set) called 'UNK', which will represent all the never seen before words in encountered in the test set.\n",
    "* Add 1 to the counts of all the words in this modified dictionary.\n",
    "* The new formula of MLE will now be: \n",
    "$$Pr_{MLE}(w_i) = \\dfrac{count(w_i) + 1}{\\Sigma_{w_j \\in V}count{(w_j)} + V}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to unigram model (add-1 smoothing): 1000/1000\n"
     ]
    }
   ],
   "source": [
    "#Implementing add-1 smoothing with unigram model. V = #(train + test) words\n",
    "\n",
    "#Adding 1 to the counts of existing entries\n",
    "for t in uni_count:\n",
    "    uni_count[t] = uni_count[t] + 1\n",
    "\n",
    "#Adding UNK token\n",
    "uni_count['UNK'] = 1\n",
    "\n",
    "V = len(uni_count)\n",
    "\n",
    "add1_uni_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    uni_mle = 1\n",
    "    for t in test_set[i][1:]:\n",
    "        if t in uni_count.keys():\n",
    "            uni_mle *= Decimal(uni_count[t])/Decimal(num_words+V)\n",
    "        else:\n",
    "            uni_mle *= Decimal(uni_count['UNK'])/Decimal(num_words+V)\n",
    "            break\n",
    "    add1_uni_mle.append(uni_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to unigram model (add-1 smoothing): {}/{}\".format(\n",
    "    np.count_nonzero(add1_uni_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Model\n",
    "* Similarly all the bigrams appearing in test set which didn't appear in the training set are represented by the 'UNK' token. Add 1 to the counts of all the tokens in our new vocabulary.\n",
    "* The new formula for MLE will now be:\n",
    "$$ Pr(w_j|w_i) = \\dfrac{count(w_i,w_j) + 1}{count(w_i) + V}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test sentences that get a non-zero probability according to bigram model (add-1 smoothing): 1000/1000\n"
     ]
    }
   ],
   "source": [
    "#Implementing add-1 smoothing with bigram model.\n",
    "\n",
    "all_test_bigrams = list(bigrams(test_words))\n",
    "\n",
    "#Adding 1 to the counts of existing entries\n",
    "for t in bi_count:\n",
    "    bi_count[t] = bi_count[t] + 1\n",
    "\n",
    "#Adding UNK token\n",
    "bi_count['UNK'] = 1\n",
    "\n",
    "add1_bi_mle = []\n",
    "for i in range(len(test_set)):\n",
    "    bi_mle = 1\n",
    "    test_bigrams = list(bigrams(test_set[i]))\n",
    "    for j in range(len(test_bigrams)):\n",
    "        if test_bigrams[j] in bi_count.keys():\n",
    "            bi_mle *= Decimal(bi_count[test_bigrams[j]])/Decimal(uni_count[test_bigrams[j][0]]+V)\n",
    "        else:\n",
    "            bi_mle *= Decimal(bi_count['UNK'])/Decimal(uni_count[test_bigrams[j][0]]+V)\n",
    "\n",
    "    add1_bi_mle.append(bi_mle)\n",
    "\n",
    "print \"# of test sentences that get a non-zero probability according to bigram model (add-1 smoothing): {}/{}\".format(\n",
    "    np.count_nonzero(add1_bi_mle), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
